{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ranges in between 0 and 1. \n",
    "\n",
    "Advantages:\n",
    "\n",
    "    + Simple and smooth curve\n",
    "    + Clear predictions\n",
    "    + Can be used in any layer including the output layer\n",
    "    \n",
    "    \n",
    "Disadvantages:\n",
    "\n",
    "    - Non-Zero centric- for large negative and positive values the output is positive and in opposite directions but \n",
    "    between [0,1] which makes it difficult to calculate gradient for such small values. \n",
    "    - Vanishing Gradient- the change in predicted values for large positive numbers is infinitesimal.\n",
    "    - The calculation is computationally complex for large networks.\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementaion of sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid of 4 is: 0.9820137900379085\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def sigmoid(z):\n",
    " return 1 / (1 + np.exp(-z))\n",
    "print(\"Sigmoid of 4 is:\",sigmoid (4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Non-zero centric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid of positive number(5) is: 0.9933071490757153\n"
     ]
    }
   ],
   "source": [
    "print(\"Sigmoid of positive number(5) is:\",sigmoid(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid of negative number(-5) is: 0.0066928509242848554\n"
     ]
    }
   ],
   "source": [
    "print(\"Sigmoid of negative number(-5) is:\",sigmoid(-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference between Derivative of Sigmoid (5) and (-5) is: -1.214306433183765e-16\n"
     ]
    }
   ],
   "source": [
    "print(\"Difference between Derivative of Sigmoid (5) and (-5) is:\", sigmoid(5)*(1- sigmoid(5))-sigmoid(-5)*(1- sigmoid(-5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For derivative of sigmoid refer \n",
    "https://math.stackexchange.com/questions/78575/derivative-of-sigmoid-function-sigma-x-frac11e-x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----> The derivatives of the outputs when compared for gradient decent will be very less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vanishing gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference between sigmoid of 14 and 15: 5.256258007735326e-07\n"
     ]
    }
   ],
   "source": [
    "print(\"Difference between sigmoid of 14 and 15:\",sigmoid(15)-sigmoid(14))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----> The difference in nearby predicted values is very less "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TanH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ranges in between -1 and 1\n",
    "\n",
    "It is a shifted version of sigmoid from [0,1] to [-1,1]\n",
    "\n",
    "Advantages:\n",
    "\n",
    "    + Zero-centric- it accommodates large positive and negative values because if calculates local (or global) minimum \n",
    "    quickly as derivatives of the tanh are larger than the derivatives of the sigmoid. It can minimize the cost function           faster.\n",
    "    \n",
    "    \n",
    "Disadvantages:\n",
    "\n",
    "    - Vanishing gradient problem\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementaion of tanH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(z):\n",
    " return np.tanh(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tanh of 4 is: 0.999329299739067\n"
     ]
    }
   ],
   "source": [
    "print(\"tanh of 4 is:\",tanh(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zero-centric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tanh of positive number(15) is: 0.9999999999998128\n"
     ]
    }
   ],
   "source": [
    "print(\"tanh of positive number(15) is:\",tanh(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tanh of positive number(-15) is: -0.9999999999998128\n"
     ]
    }
   ],
   "source": [
    "print(\"tanh of positive number(-15) is:\",tanh(-15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----> The output is far from each other hence eas to calulate the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference between tanh of 14 and 15: 1.1957101975212936e-12\n"
     ]
    }
   ],
   "source": [
    "#vanishing gradient\n",
    "\n",
    "print(\"Difference between tanh of 14 and 15:\",np.tanh(15)-np.tanh(14))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----> The difference in nearby predicted values is very less "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU(Rectified Linear Unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ranges in between 0 to max(x)\n",
    "\n",
    "Condition :\n",
    "\n",
    "           0   X < 0\n",
    "           X  otherwise\n",
    "Advantages:\n",
    "    \n",
    "    + Simple and computationally efficent \n",
    "    + No vanishing gradient problem- as the input remains the same\n",
    "    + Non-linear \n",
    "    + Sparsity- increases speed of the model by removing unwanted features(Most of the times)\n",
    "    \n",
    "Disadvantages:\n",
    "    \n",
    "    - Dead Neurons- the gradient(slope) in the negative region is 0 deactivates the neurons which cannot be changed during            backpropagation and optimization.\n",
    "    - Cannot be used as the activation function for final layer.\n",
    "           \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementaion of ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "  return max(0, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU of 10 is : 10\n"
     ]
    }
   ],
   "source": [
    "z= 10\n",
    "print(\"ReLU of \"+str(z)+\" is :\",relu(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU of -0.4 is : 0\n",
      "ReLU of -50 is : 0\n"
     ]
    }
   ],
   "source": [
    "# Dead neuron\n",
    "z= -0.4\n",
    "print(\"ReLU of \"+str(z)+\" is :\",relu(z)) \n",
    "z= -50\n",
    "print(\"ReLU of \"+str(z)+\" is :\",z * (z > 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----> all the neurons valued from [0,-inf) are deactivated which may harm the model during backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varients of ReLU\n",
    "\n",
    "    Leaky ReLU \n",
    "    Parametric ReLU  \n",
    "    Exponential ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leaky ReLU \n",
    "\n",
    "The only differnce from ReLU is replacing the negative values with constant(=0.01) times the value instead of 0. \n",
    "\n",
    "Condition :\n",
    "\n",
    "           0.01   X < 0\n",
    "           X  otherwise\n",
    "\n",
    "\n",
    "Advantange:\n",
    "    \n",
    "    + Prevents the dead neurons problem- due to the replacement of negative values the neurons do not deactive and block \n",
    "     from active backpropagation.\n",
    "    \n",
    "Disadvantage:\n",
    "      \n",
    "    -  Output not constant ## check\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementaion of Leaky ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leakyrelu(z):\n",
    "  return np.maximum(0.01 * z, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU of 10 is : 10.0\n"
     ]
    }
   ],
   "source": [
    "z= 10\n",
    "print(\"ReLU of \"+str(z)+\" is :\",leakyrelu(z)) #positive value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU of -1 is : -0.01\n"
     ]
    }
   ],
   "source": [
    "z= -1\n",
    "print(\"ReLU of \"+str(z)+\" is :\",leakyrelu(z)) #negative number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parametric ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to Leaky ReLU but replaces the negative values with variable paramater(α) times the value instead of 0\n",
    "\n",
    "Condition :\n",
    "\n",
    "       α   X < 0\n",
    "       X  otherwise\n",
    "\n",
    "Advantange:\n",
    "    \n",
    "    + Allows the negative slope to be learned— unlike leaky ReLU, this function provides the slope of the negative part \n",
    "    which helps in finding the most appropriate value of α."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementaion of Parametric ReLU\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parmetricrelu(z,α):\n",
    "  return np.maximum(α * z, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU of -1 is : 10.0\n",
      "ReLU of -1 is : -0.5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"ReLU of \"+str(z)+\" is :\",parmetricrelu(10,0.5)) #positive value\n",
    "\n",
    "print(\"ReLU of \"+str(z)+\" is :\",parmetricrelu(-1,0.5)) #negative number\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exponential ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only differnce from ReLU is replacing the negative values with exponent times the value instead of 0.\n",
    "Condition :\n",
    "\n",
    "       (e^x-1)   X < 0\n",
    "       X  otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def erelu(z,alpha):\n",
    "    return z if z >= 0 else alpha*(np.exp(z) -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exponential ReLu for 10 is : 10\n",
      "Exponential ReLu for -10 is : -2.9998638002107123\n"
     ]
    }
   ],
   "source": [
    "print(\"Exponential ReLu for 10 is :\",erelu(10,3))\n",
    "print(\"Exponential ReLu for -10 is :\",erelu(-10,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a form of logistic regression that normalizes an input value into a vector of values that follows a probability distribution whose total sums up to 1\n",
    "Advantange:\n",
    "    + Multi-dimensional classification\n",
    "    + Generally used as output neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax of 1,2 and 3 is: [0.09003057 0.24472847 0.66524096]\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    ex = np.exp(x)\n",
    "    sum_ex = np.sum( np.exp(x))\n",
    "    return ex/sum_ex\n",
    "\n",
    "\n",
    "print (\"softmax of 1,2 and 3 is:\",softmax([1,2,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Swish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Swish is a new, self-gated activation function discovered by researchers at Google to achieve better performance compared to ReLU \n",
    "\n",
    "A notable thing is that the β scaling factor is introduced in Swish.\n",
    "\n",
    "If β=1, then activation function is called the Sigmoid-weighted Linear Unit (SiL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referneces:\n",
    "\n",
    "- https://towardsdatascience.com/activation-functions-and-its-types-which-is-better-a9a5310cc8f\n",
    "\n",
    "- https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THANK YOU FOR READING. UPVOTE IF YOU FIND IT USEFUL\n",
    "****"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
